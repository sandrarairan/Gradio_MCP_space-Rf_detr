{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtWvZuZGZqmD/UpeJ/+JM6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandrarairan/Gradio_MCP_space-Rf_detr/blob/main/Gradio_MCP_space_Rf_detr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5LHAYJvNpiP"
      },
      "outputs": [],
      "source": [
        "!pip install -qU rfdetr supervision gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRADIO"
      ],
      "metadata": {
        "id": "RcVsB2s-OhAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/blog/gradio-mcp"
      ],
      "metadata": {
        "id": "_o11zyJWPFEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from typing import TypeVar, Tuple\n",
        "import numpy as np\n",
        "from rfdetr import RFDETRBase, RFDETRLarge\n",
        "from rfdetr.util.coco_classes import COCO_CLASSES\n",
        "import supervision as sv\n",
        "from rfdetr.detr import RFDETR\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "import shutil\n",
        "import uuid\n",
        "\n",
        "\n",
        "def create_directory(directory_path: str) -> None:\n",
        "    if not os.path.exists(directory_path):\n",
        "        os.makedirs(directory_path)\n",
        "\n",
        "\n",
        "def delete_directory(directory_path: str) -> None:\n",
        "    if not os.path.exists(directory_path):\n",
        "        raise FileNotFoundError(f\"Directory '{directory_path}' does not exist.\")\n",
        "\n",
        "    try:\n",
        "        shutil.rmtree(directory_path)\n",
        "    except PermissionError:\n",
        "        raise PermissionError(\n",
        "            f\"Permission denied: Unable to delete '{directory_path}'.\")\n",
        "\n",
        "\n",
        "def generate_unique_name():\n",
        "    current_datetime = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "    unique_id = uuid.uuid4()\n",
        "    return f\"{current_datetime}_{unique_id}\"\n",
        "\n",
        "MAX_VIDEO_LENGTH_SECONDS = 5\n",
        "VIDEO_SCALE_FACTOR = 0.5\n",
        "VIDEO_TARGET_DIRECTORY = \"tmp\"\n",
        "\n",
        "create_directory(directory_path=VIDEO_TARGET_DIRECTORY)\n",
        "\n",
        "def video_processing_inference(\n",
        "        input_video: str,\n",
        "        confidence: float,\n",
        "        resolution: int,\n",
        "        checkpoint: str,\n",
        "        progress=gr.Progress(track_tqdm=True)\n",
        "):\n",
        "    model = load_model(resolution=resolution, checkpoint=checkpoint)\n",
        "\n",
        "    name = generate_unique_name()\n",
        "    output_video = os.path.join(VIDEO_TARGET_DIRECTORY, f\"{name}.mp4\")\n",
        "\n",
        "    video_info = sv.VideoInfo.from_video_path(input_video)\n",
        "    video_info.width = int(video_info.width * VIDEO_SCALE_FACTOR)\n",
        "    video_info.height = int(video_info.height * VIDEO_SCALE_FACTOR)\n",
        "\n",
        "    total = min(video_info.total_frames, video_info.fps * MAX_VIDEO_LENGTH_SECONDS)\n",
        "    frames_generator = sv.get_video_frames_generator(input_video, end=total)\n",
        "\n",
        "    with sv.VideoSink(output_video, video_info=video_info) as sink:\n",
        "        for frame in tqdm(frames_generator, total=total):\n",
        "            annotated_frame = detect_and_annotate(\n",
        "                model=model,\n",
        "                image=frame,\n",
        "                confidence=confidence\n",
        "            )\n",
        "            annotated_frame = sv.scale_image(annotated_frame, VIDEO_SCALE_FACTOR)\n",
        "            sink.write_frame(annotated_frame)\n",
        "\n",
        "    return output_video\n",
        "\n",
        "\n",
        "## imagen\n",
        "\n",
        "ImageType = TypeVar(\"ImageType\", Image.Image, np.ndarray)\n",
        "\n",
        "COLOR = sv.ColorPalette.from_hex([\n",
        "    \"#ffff00\", \"#ff9b00\", \"#ff8080\", \"#ff66b2\", \"#ff66ff\", \"#b266ff\",\n",
        "    \"#9999ff\", \"#3399ff\", \"#66ffff\", \"#33ff99\", \"#66ff66\", \"#99ff00\"\n",
        "])\n",
        "\n",
        "\n",
        "def calculate_resolution_wh(image: ImageType) -> Tuple[int, int]:\n",
        "    if isinstance(image, Image.Image):\n",
        "        return image.size\n",
        "    elif isinstance(image, np.ndarray):\n",
        "        if image.ndim >= 2:\n",
        "            h, w = image.shape[:2]\n",
        "            return w, h\n",
        "        else:\n",
        "            raise ValueError(\"Input numpy array image must have at least 2 dimensions (height, width).\")\n",
        "    else:\n",
        "        raise TypeError(\"Input image must be a Pillow Image or a numpy array.\")\n",
        "\n",
        "\n",
        "def detect_and_annotate(\n",
        "        model: RFDETR,\n",
        "        image: ImageType,\n",
        "        confidence: float\n",
        ") -> ImageType:\n",
        "    detections = model.predict(image, threshold=confidence)\n",
        "\n",
        "    resolution_wh = calculate_resolution_wh(image)\n",
        "    text_scale = sv.calculate_optimal_text_scale(resolution_wh=resolution_wh) - 0.4\n",
        "    thickness = sv.calculate_optimal_line_thickness(resolution_wh=resolution_wh)-2\n",
        "\n",
        "    bbox_annotator = sv.BoxAnnotator(color=COLOR, thickness=thickness)\n",
        "    label_annotator = sv.LabelAnnotator(\n",
        "        color=COLOR,\n",
        "        text_color=sv.Color.BLACK,\n",
        "        text_scale=text_scale,\n",
        "        text_padding=1\n",
        "\n",
        "    )\n",
        "\n",
        "    labels = [\n",
        "        f\"{COCO_CLASSES[class_id]} {conf:.2f}\"\n",
        "        for class_id, conf in zip(detections.class_id, detections.confidence)\n",
        "    ]\n",
        "\n",
        "    annotated_image = image.copy()\n",
        "    annotated_image = bbox_annotator.annotate(annotated_image, detections)\n",
        "    annotated_image = label_annotator.annotate(annotated_image, detections, labels)\n",
        "    return annotated_image\n",
        "\n",
        "\n",
        "def load_model(resolution: int, checkpoint: str) -> RFDETR:\n",
        "    if checkpoint == \"base\":\n",
        "        return RFDETRBase(resolution=resolution)\n",
        "    elif checkpoint == \"large\":\n",
        "        return RFDETRLarge(resolution=resolution)\n",
        "    raise TypeError(\"Checkpoint must be 'base' or 'large'\")\n",
        "\n",
        "\n",
        "def image_processing_inference(\n",
        "        input_image: Image.Image,\n",
        "        confidence: float,\n",
        "        resolution: int,\n",
        "        checkpoint: str\n",
        "):\n",
        "    input_image = input_image.resize((resolution, resolution))  # Asegura tama침o correcto\n",
        "    model = load_model(resolution=resolution, checkpoint=checkpoint)\n",
        "    return detect_and_annotate(model=model, image=input_image, confidence=confidence)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ke_SfoqKOcq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interfaz Gradio\n",
        "def gradio_interface(image, confidence, resolution, checkpoint):\n",
        "    return image_processing_inference(\n",
        "        input_image=image,\n",
        "        confidence=confidence,\n",
        "        resolution=resolution,\n",
        "        checkpoint=checkpoint\n",
        "    )\n",
        "\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"pil\", label=\"Sube una imagen\"),\n",
        "        gr.Slider(0.1, 1.0, value=0.5, step=0.05, label=\"Umbral de confianza\"),\n",
        "        gr.Slider(320, 1400, step=8, value=728, label=\"Resoluci칩n de entrada\"),\n",
        "        gr.Radio(choices=[\"base\", \"large\"], value=\"base\", label=\"Modelo (checkpoint)\")\n",
        "    ],\n",
        "    outputs=gr.Image(type=\"pil\", label=\"Resultado con detecciones\"),\n",
        "    title=\"Demo RF-DETR con Gradio\",\n",
        "    description=\"Sube una imagen y selecciona el modelo, resoluci칩n y umbral de confianza para realizar detecci칩n de objetos.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(mcp_server=True)"
      ],
      "metadata": {
        "id": "Tot6HdyYOphJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}